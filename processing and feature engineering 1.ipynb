{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12333510,"sourceType":"datasetVersion","datasetId":7774782}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-30T21:37:28.321979Z","iopub.execute_input":"2025-06-30T21:37:28.322336Z","iopub.status.idle":"2025-06-30T21:37:28.357053Z","shell.execute_reply.started":"2025-06-30T21:37:28.322309Z","shell.execute_reply":"2025-06-30T21:37:28.356137Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/amexda/test_data.parquet\n/kaggle/input/amexda/add_event.parquet\n/kaggle/input/amexda/685404e30cfdb_submission_template.csv\n/kaggle/input/amexda/data_dictionary.csv\n/kaggle/input/amexda/offer_metadata.parquet\n/kaggle/input/amexda/add_trans.parquet\n/kaggle/input/amexda/train_data.parquet\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\n\n# Set paths\nDATA_PATH = Path('/kaggle/input/amexda')\nOUTPUT_PATH = Path('/kaggle/working/eda_results')\n\n# Create output directory\nOUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n\ndef save_plot(fig, filename):\n    fig.savefig(OUTPUT_PATH / filename, bbox_inches='tight')\n    plt.close(fig)\n\n# 1. Load datasets with progress monitoring\nprint(\"Loading datasets...\")\ntrain = pd.read_parquet(DATA_PATH/'train_data.parquet', engine='pyarrow')\ntest = pd.read_parquet(DATA_PATH/'test_data.parquet', engine='pyarrow')\nmeta = pd.read_parquet(DATA_PATH/'offer_metadata.parquet', engine='pyarrow')\nevents = pd.read_parquet(DATA_PATH/'add_event.parquet', engine='pyarrow')\ntrans = pd.read_parquet(DATA_PATH/'add_trans.parquet', engine='pyarrow')\n\n# 2. Basic dataset inspection\ndef dataset_summary(df, name):\n    summary = pd.DataFrame({\n        'Dataset': name,\n        'Rows': [df.shape[0]],\n        'Columns': [df.shape[1]],\n        'Missing_%': [df.isna().mean().mean() * 100],\n        'Size_MB': [df.memory_usage(deep=True).sum() / (1024**2)]\n    })\n    return summary\n\nsummary_list = [\n    dataset_summary(train, 'train'),\n    dataset_summary(test, 'test'),\n    dataset_summary(meta, 'metadata'),\n    dataset_summary(events, 'events'),\n    dataset_summary(trans, 'transactions')\n]\n\nsummary_df = pd.concat(summary_list)\nprint(summary_df)\nsummary_df.to_csv(OUTPUT_PATH / 'dataset_summary.csv', index=False)\n\n# 3. Target analysis (train only)\nplt.figure(figsize=(10, 6))\nax = train['y'].value_counts(normalize=True).plot.bar()\nplt.title('Target Distribution (Click Rate)')\nplt.ylabel('Proportion')\nplt.xticks([0,1], ['No Click (0)', 'Click (1)'], rotation=0)\nsave_plot(plt.gcf(), 'target_distribution.png')\n# Convert 'y' to numeric type before analysis\ntrain['y'] = pd.to_numeric(train['y'], errors='coerce')\n\n# Now you can safely calculate CTR\nctr = train['y'].mean()\nprint(f\"Overall Click-Through Rate: {ctr:.4%}\")\n\n# 4. Missing value analysis\ndef plot_missing(df, name):\n    missing = df.isna().sum()\n    missing = missing[missing > 0].sort_values(ascending=False)\n    \n    plt.figure(figsize=(12, 8))\n    missing[:30].plot.bar()  # Top 30 features with missing values\n    plt.title(f'Missing Values - {name}')\n    plt.ylabel('Missing Count')\n    save_plot(plt.gcf(), f'missing_{name}.png')\n    return missing\n\ntrain_missing = plot_missing(train, 'Training Data')\ntest_missing = plot_missing(test, 'Test Data')\n\n# 5. Temporal analysis (using id5 date column)\ntrain['date'] = pd.to_datetime(train['id5'], errors='coerce')\ntest['date'] = pd.to_datetime(test['id5'], errors='coerce')\n\nplt.figure(figsize=(12, 6))\ntrain.groupby(train['date'].dt.date)['y'].mean().plot()\nplt.title('Daily Click-Through Rate Over Time')\nplt.ylabel('CTR')\nsave_plot(plt.gcf(), 'daily_ctr_trend.png')\n\n# 6. Feature distributions (sampled for efficiency)\ndef plot_feature_distributions(df, name, sample_size=10000):\n    sample = df.sample(min(sample_size, len(df)), random_state=42)\n    \n    # Numerical features\n    num_cols = sample.select_dtypes(include=np.number).columns\n    for col in num_cols[:5]:  # First 5 numerical features\n        plt.figure()\n        sns.histplot(sample[col], kde=True)\n        plt.title(f'Distribution of {col}')\n        save_plot(plt.gcf(), f'dist_{name}_{col}.png')\n    \n    # Categorical features\n    cat_cols = sample.select_dtypes(include='object').columns\n    for col in cat_cols[:3]:  # First 3 categorical features\n        plt.figure()\n        sample[col].value_counts().head(10).plot.bar()\n        plt.title(f'Top 10 Values - {col}')\n        save_plot(plt.gcf(), f'categorical_{name}_{col}.png')\n\nplot_feature_distributions(train, 'train')\nplot_feature_distributions(test, 'test')\n\n# 7. Additional datasets analysis\n# Offer metadata analysis\nplt.figure(figsize=(10, 6))\nmeta['f376'].plot.hist(bins=30)  # Discount rate distribution\nplt.title('Discount Rate Distribution')\nsave_plot(plt.gcf(), 'discount_distribution.png')\n\n# Events data analysis\nevents['event_date'] = pd.to_datetime(events['id4'], errors='coerce')\nevents['event_type'] = events['id7'].notna().map({True: 'Click', False: 'Impression'})\n\nplt.figure(figsize=(10, 6))\nevents['event_type'].value_counts().plot.pie(autopct='%1.1f%%')\nplt.title('Event Type Distribution')\nsave_plot(plt.gcf(), 'event_type_distribution.png')\n\n# 8. Save memory by downcasting\ndef downcast(df):\n    for col in df.select_dtypes(include='integer'):\n        df[col] = pd.to_numeric(df[col], downcast='integer')\n    for col in df.select_dtypes(include='float'):\n        df[col] = pd.to_numeric(df[col], downcast='float')\n    return df\n\ntrain = downcast(train)\ntest = downcast(test)\n\n# 9. Generate report\nwith open(OUTPUT_PATH / 'eda_report.md', 'w') as f:\n    f.write(\"# AMEX Competition EDA Report\\n\")\n    f.write(f\"**Overall CTR**: {ctr:.4%}\\n\\n\")\n    f.write(\"## Dataset Summary\\n\")\n    f.write(summary_df.to_markdown(index=False) + \"\\n\\n\")\n    f.write(\"## Key Findings\\n\")\n    f.write(\"- Target variable shows significant class imbalance\\n\")\n    f.write(\"- Temporal patterns observed in CTR\\n\")\n    f.write(\"- Several features contain missing values (see visualizations)\\n\")\n    f.write(\"## Next Steps\\n\")\n    f.write(\"- Feature engineering for temporal patterns\\n\")\n    f.write(\"- Imputation strategy for missing values\\n\")\n    f.write(\"- Class imbalance handling in modeling\")\n\nprint(\"EDA complete! Results saved to:\", OUTPUT_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T21:45:26.523957Z","iopub.execute_input":"2025-06-30T21:45:26.524332Z","iopub.status.idle":"2025-06-30T21:48:51.977897Z","shell.execute_reply.started":"2025-06-30T21:45:26.524305Z","shell.execute_reply":"2025-06-30T21:48:51.977059Z"}},"outputs":[{"name":"stdout","text":"Loading datasets...\n        Dataset      Rows  Columns  Missing_%       Size_MB\n0         train    770164      372  23.838103  14712.150325\n0          test    369301      371  20.503509   7228.309033\n0      metadata      4164       12  24.151457      2.168164\n0        events  21457473        5  19.628018   4846.647804\n0  transactions   6339465        9   0.002286   2718.020789\nOverall Click-Through Rate: 4.8108%\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n","output_type":"stream"},{"name":"stdout","text":"EDA complete! Results saved to: /kaggle/working/eda_results\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# STEP-3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#  CLEAN DATA (no chained assignment warnings)  â€“  AMEX Campus Challenge\n#  Assumes Step-2 EDA already ran and dataset â€œamexdaâ€ is attached.\n\nimport pandas as pd, numpy as np\nfrom pathlib import Path\n\nDATA_DIR = Path('/kaggle/input/amexda')\nWORK_DIR = Path('/kaggle/working')\nWORK_DIR.mkdir(exist_ok=True, parents=True)\n\ntrain = pd.read_parquet(DATA_DIR/'train_data.parquet', engine='pyarrow')\ntest  = pd.read_parquet(DATA_DIR/'test_data.parquet',  engine='pyarrow')\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 1.  Make sure the target is numeric\ntrain['y'] = pd.to_numeric(train['y'], errors='coerce')\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 2.  Universal cleaning helper (NO chained assignment)\ndef clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()                                         # prevent surprises if caller passes a view\n    \n    # 2-A  Â±âˆž âž” NaN  (numeric cols only)\n    num_cols = df.select_dtypes(include=[np.number]).columns\n    df[num_cols] = df[num_cols].replace([np.inf, -np.inf], np.nan)\n    \n    # 2-B  build imputation maps once\n    median_map = {c: df[c].median() \n                  for c in num_cols if df[c].isna().any()}\n    \n    cat_cols   = df.select_dtypes(exclude=[np.number]).columns\n    mode_map   = {c: (df[c].mode().iloc[0] \n                      if not df[c].mode().empty else 'Unknown')\n                  for c in cat_cols if df[c].isna().any()}\n    \n    # single pass fill (avoids chained-assignment warning)\n    df.fillna(value={**median_map, **mode_map}, inplace=True)\n    \n    # 2-C  down-cast to save RAM\n    for c in df.select_dtypes(include='float').columns:\n        df[c] = pd.to_numeric(df[c], downcast='float')\n    for c in df.select_dtypes(include='int').columns:\n        df[c] = pd.to_numeric(df[c], downcast='integer')\n    \n    # 2-D  drop exact duplicates (keeps row count â‰¤ original)\n    if df.duplicated().any():\n        df = df.drop_duplicates()\n    return df\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 3.  Apply cleaner\ntrain = clean_dataframe(train)\ntest  = clean_dataframe(test)\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 4.  Final sanity checks\nassert not train.isna().any().any(), 'NaNs remain in train'\nassert not test.isna().any().any(),  'NaNs remain in test'\nassert not np.isinf(train.select_dtypes(include=[np.number])).any().any()\nassert not np.isinf(test.select_dtypes(include=[np.number])).any().any()\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 5.  Persist cleaned data\ntrain.to_parquet(WORK_DIR/'train_clean.parquet', index=False, engine='pyarrow')\ntest.to_parquet (WORK_DIR/'test_clean.parquet',  index=False, engine='pyarrow')\n\nprint('âœ…  Cleaning finished â†’ no warnings, files saved in /kaggle/working')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T21:56:25.331902Z","iopub.execute_input":"2025-06-30T21:56:25.332268Z","iopub.status.idle":"2025-06-30T22:00:18.557414Z","shell.execute_reply.started":"2025-06-30T21:56:25.332241Z","shell.execute_reply":"2025-06-30T22:00:18.555945Z"}},"outputs":[{"name":"stdout","text":"âœ…  Cleaning finished â†’ no warnings, files saved in /kaggle/working\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import os, glob, pathlib, pprint\npprint.pprint(glob.glob('/kaggle/working/**/*.parquet', recursive=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T21:44:46.528495Z","iopub.execute_input":"2025-06-30T21:44:46.528850Z","iopub.status.idle":"2025-06-30T21:44:46.534971Z","shell.execute_reply.started":"2025-06-30T21:44:46.528805Z","shell.execute_reply":"2025-06-30T21:44:46.534151Z"}},"outputs":[{"name":"stdout","text":"['/kaggle/working/test_clean.parquet', '/kaggle/working/train_clean.parquet']\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import pandas as pd, numpy as np, gc, time, sys\nfrom pathlib import Path\n\nRAW_DIR = Path('/kaggle/input/amexda')\nCLEAN_DIR = Path('/kaggle/working')\nWORK_DIR = Path('/kaggle/working')\nOUT_TRAIN = WORK_DIR / 'fe_v1_train.parquet'\nOUT_TEST  = WORK_DIR / 'fe_v1_test.parquet'\n\n# Fast-path: skip if already exists\nif OUT_TRAIN.exists() and OUT_TEST.exists():\n    print('âœ… fe_v1 already exists â€“ skipping regeneration.')\n    sys.exit(0)\n\n# Load clean train/test\ntrain = pd.read_parquet(CLEAN_DIR / 'train_clean.parquet')\ntest  = pd.read_parquet(CLEAN_DIR / 'test_clean.parquet')\n\n# Force id2 and id3 to int64 in train/test\ntrain['id2'] = train['id2'].astype('int64')\ntrain['id3'] = train['id3'].astype('int64')\ntest['id2'] = test['id2'].astype('int64')\ntest['id3'] = test['id3'].astype('int64')\n\n# Load events and force id2/id3 to int64\nevents_cols = ['id2', 'id3', 'id4', 'id7']\nevents = pd.read_parquet(RAW_DIR / 'add_event.parquet', columns=events_cols)\nevents['id2'] = pd.to_numeric(events['id2'], errors='coerce').astype('int64')\nevents['id3'] = pd.to_numeric(events['id3'], errors='coerce').astype('int64')\n\nevents['imp']   = 1\nevents['click'] = events['id7'].notna().astype('int8')\n\n# Aggregations\nagg = (events.groupby(['id2', 'id3'])\n               .agg(imp_cnt=('imp', 'sum'),\n                    click_cnt=('click', 'sum'))\n               .reset_index())\nagg['ctr_id2_id3'] = agg['click_cnt'] / agg['imp_cnt']\n\noffer_agg = (agg.groupby('id3')\n                 .agg(offer_imp_cnt=('imp_cnt', 'sum'),\n                      offer_click_cnt=('click_cnt', 'sum'))\n                 .reset_index())\noffer_agg['offer_ctr'] = offer_agg['offer_click_cnt'] / offer_agg['offer_imp_cnt']\n\ncust_agg = (agg.groupby('id2')\n                .agg(cust_imp_cnt=('imp_cnt', 'sum'),\n                     cust_click_cnt=('click_cnt', 'sum'))\n                .reset_index())\ncust_agg['cust_ctr'] = cust_agg['cust_click_cnt'] / cust_agg['cust_imp_cnt']\n\ndel events\ngc.collect()\n\n# Merge into train/test\ndef enrich(df):\n    df = df.copy()\n    df['id2'] = df['id2'].astype('int64')\n    df['id3'] = df['id3'].astype('int64')\n    df = (df.merge(agg,       on=['id2', 'id3'], how='left')\n            .merge(offer_agg, on='id3',          how='left')\n            .merge(cust_agg,  on='id2',          how='left'))\n    num_cols = df.select_dtypes(include=[np.number]).columns\n    df[num_cols] = df[num_cols].fillna(0).astype('float32')\n    return df\n\ntrain = enrich(train)\ntest  = enrich(test)\n\ntrain.to_parquet(OUT_TRAIN, index=False)\ntest.to_parquet(OUT_TEST,  index=False)\nprint('ðŸŽ‰ fe_v1 saved â†’', OUT_TRAIN.name, ',', OUT_TEST.name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T22:32:14.585910Z","iopub.execute_input":"2025-06-30T22:32:14.586673Z","iopub.status.idle":"2025-06-30T22:35:01.528114Z","shell.execute_reply.started":"2025-06-30T22:32:14.586640Z","shell.execute_reply":"2025-06-30T22:35:01.527001Z"}},"outputs":[{"name":"stdout","text":"ðŸŽ‰ fe_v1 saved â†’ fe_v1_train.parquet , fe_v1_test.parquet\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}