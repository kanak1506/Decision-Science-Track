{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12338866,"sourceType":"datasetVersion","datasetId":7775507}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom pathlib import Path\nimport pickle\nimport sys\n\nFEAT_DIR = Path('/kaggle/input/wdwfdws')\nFE_TRAIN = FEAT_DIR / 'fe_v2_train.parquet'\nFE_TEST  = FEAT_DIR / 'fe_v2_test.parquet'\nWORK_DIR = Path('/kaggle/working')\nMODEL_PKL = WORK_DIR / 'lgbm_baseline.pkl'\nOOF_CSV   = WORK_DIR / 'oof_preds.csv'\nTEST_CSV  = WORK_DIR / 'test_preds.csv'\n\n# Fast-path: skip if already trained\nif MODEL_PKL.exists() and TEST_CSV.exists():\n    print('Baseline model already trained â€“ skipping.')\n    sys.exit(0)\n\ntrain = pd.read_parquet(FE_TRAIN)\ntest  = pd.read_parquet(FE_TEST)\n\nTARGET = 'y'\nID_COLS = ['id1', 'id2', 'id3', 'id5']\n\ny = train[TARGET].astype('float32')\nX = train.drop(columns=[TARGET] + [col for col in ID_COLS if col in train.columns])\nX_test = test.drop(columns=[col for col in ID_COLS if col in test.columns])\n\n# Ensure all columns are numeric\nfor df in [X, X_test]:\n    for col in df.columns:\n        df[col] = pd.to_numeric(df[col], errors='coerce')\n\nbad_cols = X.select_dtypes(exclude=['int', 'float', 'bool']).columns\nassert len(bad_cols) == 0, f\"Non-numeric columns remain: {bad_cols}\"\n\n# MAP@7 scorer\ndef apk(actual, pred, k=7):\n    if len(pred) > k: pred = pred[:k]\n    score, num_hits = 0.0, 0\n    for i, p in enumerate(pred, start=1):\n        if p in actual and p not in pred[:i-1]:\n            num_hits += 1\n            score += num_hits / i\n    return score / min(len(actual), k) if actual else 0.0\n\ndef mapk(df, k=7):\n    scores = []\n    for cm, grp in df.groupby('id2'):\n        true_offers = grp.loc[grp.y_true == 1, 'id3'].tolist()\n        pred_offers = grp.sort_values('y_pred', ascending=False)['id3'].tolist()\n        scores.append(apk(true_offers, pred_offers, k))\n    return np.mean(scores)\n\nparams = dict(\n    objective='binary',\n    metric='binary_logloss',  # required for early stopping\n    learning_rate=0.05,\n    num_leaves=64,\n    feature_fraction=0.8,\n    bagging_fraction=0.8,\n    bagging_freq=1,\n    max_depth=-1,\n    verbose=-1,\n    n_estimators=1000\n)\n\nn_splits = 4\nfolds = KFold(n_splits=n_splits, shuffle=False)\n\noof_pred = np.zeros(len(train), dtype='float32')\ntest_pred = np.zeros(len(test), dtype='float32')\n\nfor fold, (tr_idx, val_idx) in enumerate(folds.split(train)):\n    print(f'fold {fold+1}/{n_splits} ...', end='')\n    dtrain = lgb.Dataset(X.iloc[tr_idx], label=y.iloc[tr_idx])\n    dval = lgb.Dataset(X.iloc[val_idx], label=y.iloc[val_idx])\n\n    clf = lgb.train(\n        params,\n        dtrain,\n        valid_sets=[dval],\n        valid_names=['val'],\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=100),\n            lgb.log_evaluation(period=100)\n        ],\n        num_boost_round=10000\n    )\n\n    oof_pred[val_idx] = clf.predict(X.iloc[val_idx], num_iteration=clf.best_iteration)\n    test_pred += clf.predict(X_test, num_iteration=clf.best_iteration) / n_splits\n    print('done')\n\nwith open(MODEL_PKL, 'wb') as f:\n    pickle.dump(clf, f)\n\noof_df = train[['id1', 'id2', 'id3']].copy()\noof_df['y_true'] = y.values\noof_df['y_pred'] = oof_pred\nscore = mapk(oof_df, k=7)\nprint(f'CV MAP@7 = {score:.5f}')\noof_df.to_csv(OOF_CSV, index=False)\n\nsub = test[['id1', 'id2', 'id3', 'id5']].copy()\nsub['pred'] = test_pred\nsub.to_csv(TEST_CSV, index=False)\nprint('Artefacts saved:', MODEL_PKL.name, OOF_CSV.name, TEST_CSV.name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:20:10.300789Z","iopub.execute_input":"2025-07-01T09:20:10.301127Z","iopub.status.idle":"2025-07-01T09:27:49.395865Z","shell.execute_reply.started":"2025-07-01T09:20:10.301102Z","shell.execute_reply":"2025-07-01T09:27:49.394661Z"}},"outputs":[{"name":"stdout","text":"fold 1/4 ...","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 100 rounds\n[100]\tval's binary_logloss: 0.116586\nEarly stopping, best iteration is:\n[87]\tval's binary_logloss: 0.116438\ndone\nfold 2/4 ...","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 100 rounds\n[100]\tval's binary_logloss: 0.107993\n[200]\tval's binary_logloss: 0.108425\nEarly stopping, best iteration is:\n[101]\tval's binary_logloss: 0.107975\ndone\nfold 3/4 ...","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 100 rounds\n[100]\tval's binary_logloss: 0.0908431\n[200]\tval's binary_logloss: 0.0903453\nEarly stopping, best iteration is:\n[194]\tval's binary_logloss: 0.0903195\ndone\nfold 4/4 ...","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 100 rounds\n[100]\tval's binary_logloss: 0.113605\nEarly stopping, best iteration is:\n[88]\tval's binary_logloss: 0.113534\ndone\nCV MAP@7 = 0.04832\nArtefacts saved: lgbm_baseline.pkl oof_preds.csv test_preds.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\n\nWORK_DIR = Path('/kaggle/working')\nTEAM_NAME = 'amex'  # <-- Replace with your team name\nTEST_PRED = WORK_DIR / 'test_preds.csv'\nSUBMISSION = WORK_DIR / f'r2_submission_{TEAM_NAME}.csv'\n\n# Read test predictions\nsub = pd.read_csv(TEST_PRED)\n\n# Ensure correct columns and order\nrequired_cols = ['id1', 'id2', 'id3', 'id5', 'pred']\nsub = sub[required_cols]\n\n# Save submission file\nsub.to_csv(SUBMISSION, index=False)\nprint(f\"Submission file saved as {SUBMISSION}\")\nprint(sub.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:39:54.874608Z","iopub.execute_input":"2025-07-01T09:39:54.875409Z","iopub.status.idle":"2025-07-01T09:39:57.699248Z","shell.execute_reply.started":"2025-07-01T09:39:54.875355Z","shell.execute_reply":"2025-07-01T09:39:57.697934Z"}},"outputs":[{"name":"stdout","text":"Submission file saved as /kaggle/working/r2_submission_amex.csv\n                                               id1        id2       id3  \\\n0   1362907_91950_16-23_2023-11-04 18:56:26.000794  1362907.0   91950.0   \n1      1082599_88356_16-23_2023-11-04 06:08:53.373  1082599.0   88356.0   \n2  1888466_958700_16-23_2023-11-05 10:07:28.000725  1888466.0  958700.0   \n3     1888971_795739_16-23_2023-11-04 12:25:28.244  1888971.0  795739.0   \n4      1256369_82296_16-23_2023-11-05 06:45:26.657  1256369.0   82296.0   \n\n          id5      pred  \n0  2023-11-04  0.005301  \n1  2023-11-04  0.012505  \n2  2023-11-05  0.943889  \n3  2023-11-04  0.006382  \n4  2023-11-05  0.006091  \n","output_type":"stream"}],"execution_count":3}]}